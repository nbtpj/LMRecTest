{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! git clone https://github.com/nbtpj/LMRecTest.git\nimport os\nos.chdir('LMRecTest/')\n# ! pip install -r requirements.txt\n! ls\n# !mkdir ./cache/\n# !cp -R /kaggle/input/movilenxamazonecomment/* ./cache/\n# !ls cache","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-15T03:20:06.460875Z","iopub.execute_input":"2023-03-15T03:20:06.461617Z","iopub.status.idle":"2023-03-15T03:20:09.630253Z","shell.execute_reply.started":"2023-03-15T03:20:06.461577Z","shell.execute_reply":"2023-03-15T03:20:09.628716Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'LMRecTest'...\nremote: Enumerating objects: 122, done.\u001b[K\nremote: Counting objects: 100% (122/122), done.\u001b[K\nremote: Compressing objects: 100% (82/82), done.\u001b[K\nremote: Total 122 (delta 75), reused 85 (delta 38), pack-reused 0\u001b[K\nReceiving objects: 100% (122/122), 32.25 KiB | 2.93 MiB/s, done.\nResolving deltas: 100% (75/75), done.\nBARTRec.py  env_config.py  metrics.py\t     runlmrec.ipynb\nGPTRec.py   make_data.py   requirements.txt  test_MLRec.py\n","output_type":"stream"}]},{"cell_type":"code","source":"# data_dir = \"/kaggle/input/movilenxamazonecomment\"\nfrom datasets import load_dataset, DatasetDict\n\nsubsets = ['movielens-1m-movies', 'movielens-1m-movies', ]\n    \nunified_dataset = DatasetDict({\n    k: load_dataset(f'nbtpj/{k}') for k in subsets\n})\nsampled_prompt_movilens_dataset = load_dataset(f'nbtpj/6k-sample')['train']\nsampled_prompt_movilens_dataset = sampled_prompt_movilens_dataset.select(range(1000))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T03:23:31.096281Z","iopub.execute_input":"2023-03-15T03:23:31.096771Z","iopub.status.idle":"2023-03-15T03:23:35.692178Z","shell.execute_reply.started":"2023-03-15T03:23:31.096726Z","shell.execute_reply":"2023-03-15T03:23:35.691087Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ade436b41944eeb82dd503c32269c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"484371dfffa94150847d93446d688789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d592763b638341ab8395d3d765a35e78"}},"metadata":{}}]},{"cell_type":"code","source":"# !python make_data.py 4000","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-15T03:23:35.694484Z","iopub.execute_input":"2023-03-15T03:23:35.695235Z","iopub.status.idle":"2023-03-15T03:23:35.699781Z","shell.execute_reply.started":"2023-03-15T03:23:35.695195Z","shell.execute_reply":"2023-03-15T03:23:35.698693Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from env_config import TERM_TO_ESTIMATE, RATINGS, movielen_feat_map\n\ntest_scenario = {\n    'biography only': ['bio prompt'],\n    'bio + top 10 nearest ratings': ['bio prompt', 'history prompt'],\n    'bio + 10 nearest ratings + history overview': ['bio prompt', 'history prompt', 'history overview prompt'],\n    'bio + 10 nearest ratings + history overview + age-group': ['bio prompt', 'history prompt', 'history overview prompt', 'age-group prompt'],\n    'bio + 10 nearest ratings + history overview + age-group + next-category': ['bio prompt', 'history prompt', 'history overview prompt', 'age-group prompt', 'cross-cate prompt'],\n}\n\ndef film_promt(film_meta: dict) -> dict:\n    title = None\n    try:\n        title = film_meta['movie_title'].decode('utf-8')\n    except:\n        pass\n    categories = []\n    try:\n        categories = [movielen_feat_map['movie_genres'][k] for k in film_meta['movie_genres']]\n    except:\n        pass\n\n    return {\n        'target prompt': f'The movie named {title} is categorized as {\", \".join(categories)}.{TERM_TO_ESTIMATE}'\n    }\n\ndef map2ctx(rating, test_scenario, targets_ids):\n    r = {}\n    for name, cols_to_gather in test_scenario.items():\n        try:\n            r[name] = [rating[col] for col in cols_to_gather]\n            r[name] = ' '.join(r[name])\n        except:\n            pass\n    r.update({\n        'user_rating': rating['user_rating'],\n        'expected': targets_ids.index(rating['movie_id'])\n    })\n    return r\n\ntarget_details = sampled_prompt_movilens_dataset.map(film_promt)\ntarget_details","metadata":{"execution":{"iopub.status.busy":"2023-03-15T03:23:35.701691Z","iopub.execute_input":"2023-03-15T03:23:35.702507Z","iopub.status.idle":"2023-03-15T03:23:36.000105Z","shell.execute_reply.started":"2023-03-15T03:23:35.702455Z","shell.execute_reply":"2023-03-15T03:23:35.999098Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccfafffe1fd74cd9babe95e179e993e1"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['bucketized_user_age', 'movie_genres', 'movie_id', 'movie_title', 'timestamp', 'user_gender', 'user_id', 'user_occupation_label', 'user_occupation_text', 'user_rating', 'user_zip_code', 'bio prompt', 'history prompt', 'history overview prompt', 'job-group prompt', 'age-group prompt', 'gender-group prompt', 'region-group prompt', 'cross-movie prompt', 'cross-cate prompt', 'target prompt'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"RATINGS","metadata":{"execution":{"iopub.status.busy":"2023-03-15T03:23:36.003035Z","iopub.execute_input":"2023-03-15T03:23:36.003519Z","iopub.status.idle":"2023-03-15T03:23:36.012868Z","shell.execute_reply.started":"2023-03-15T03:23:36.003478Z","shell.execute_reply":"2023-03-15T03:23:36.011794Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"['This user rate this movie 1 out of 5.',\n 'This user rate this movie 2 out of 5.',\n 'This user rate this movie 3 out of 5.',\n 'This user rate this movie 4 out of 5.',\n 'This user rate this movie 5 out of 5.']"},"metadata":{}}]},{"cell_type":"code","source":"from env_config import device\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-03-15T03:23:36.014300Z","iopub.execute_input":"2023-03-15T03:23:36.015357Z","iopub.status.idle":"2023-03-15T03:23:36.027272Z","shell.execute_reply.started":"2023-03-15T03:23:36.015324Z","shell.execute_reply":"2023-03-15T03:23:36.026106Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'cuda:0'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test Recommender Scenario\n___","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ntargets = target_details['target prompt'] # all possible selections\ntargets_ids = target_details['movie_id']\ncontext_selection = sampled_prompt_movilens_dataset.map(map2ctx, \n                                                        fn_kwargs = {'test_scenario':test_scenario, 'targets_ids': targets_ids},\n                                                        remove_columns=sampled_prompt_movilens_dataset.column_names)\nexpected = np.array(context_selection['expected'])\ncontext_selection","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-03-15T03:23:36.028837Z","iopub.execute_input":"2023-03-15T03:23:36.029117Z","iopub.status.idle":"2023-03-15T03:23:36.379023Z","shell.execute_reply.started":"2023-03-15T03:23:36.029091Z","shell.execute_reply":"2023-03-15T03:23:36.378036Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6205b96a5c749a8af30e93625ecca16"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['user_rating', 'biography only', 'bio + top 10 nearest ratings', 'bio + 10 nearest ratings + history overview', 'bio + 10 nearest ratings + history overview + age-group', 'bio + 10 nearest ratings + history overview + age-group + next-category', 'expected'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"%env CUDA_LAUNCH_BLOCKING=1\nfrom transformers import BartTokenizer, BartForConditionalGeneration, GPT2Tokenizer, GPT2LMHeadModel\nfrom metrics import *\nfrom tqdm import tqdm\nfrom BARTRec import rank_with_bart\nfrom GPTRec import rank_with_gpt\n\navailable_selections = target_details['target prompt']\nrank_method_names = ['BART', 'GPT']\nrank_methods = [rank_with_bart, rank_with_gpt]\nmodels = [BartForConditionalGeneration.from_pretrained('facebook/bart-base').to(device), GPT2LMHeadModel.from_pretrained('gpt2').to(device)]\ntokenizers = [BartTokenizer.from_pretrained('facebook/bart-base'), GPT2Tokenizer.from_pretrained('gpt2')]\ntokenizers[-1].add_special_tokens({'pad_token': '[PAD]'})\nsize = 20\nresults = []\ntotal = len(test_scenario) * len(rank_methods) * len(context_selection)\nwith tqdm(test_scenario, total=total, desc='Processing') as process:\n    for scenario in process:\n        if scenario in context_selection.column_names:\n            contexts = context_selection[scenario]\n            for rank_method, model, tokenizer, name in zip(rank_methods, models, tokenizers, rank_method_names):\n                predictions = rank_method(model=model, tokenizer=tokenizer,\n                                    contexts=contexts, available_selections=available_selections,\n                                    term_to_estimate = TERM_TO_ESTIMATE,\n                                    verbose=process,\n                                    batch_size=16)\n                eval_rs = MMRandR(predictions, expected, size=20)\n                results.append({'model': name, 'scenario': scenario, **eval_rs})\nlen(results)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-15T03:24:37.766416Z","iopub.execute_input":"2023-03-15T03:24:37.766811Z","iopub.status.idle":"2023-03-15T03:26:06.102152Z","shell.execute_reply.started":"2023-03-15T03:24:37.766770Z","shell.execute_reply":"2023-03-15T03:26:06.099941Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"env: CUDA_LAUNCH_BLOCKING=1\n","output_type":"stream"},{"name":"stderr","text":"Processing:   0%|          | 0/10000 [01:20<?, ?it/s]           \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2105826823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                     \u001b[0mterm_to_estimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERM_TO_ESTIMATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                     batch_size=16)\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0meval_rs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMMRandR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scenario'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_rs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/LMRecTest/BARTRec.py\u001b[0m in \u001b[0;36mrank_with_bart\u001b[0;34m(model, tokenizer, contexts, available_selections, decoder_embeddings, context_embedding, term_to_estimate, batch_size, disable_paralell, verbose)\u001b[0m\n\u001b[1;32m    153\u001b[0m         log_p = predict_log_prob(ctx_embedding, **decoder_embeddings, \n\u001b[1;32m    154\u001b[0m                                  \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                                  disable_paralell=disable_paralell)\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mright_position_in_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_selections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_position_in_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/LMRecTest/BARTRec.py\u001b[0m in \u001b[0;36mpredict_log_prob\u001b[0;34m(context, decoder_inputs_embeds, decoder_attention_mask, labels, model, batch_size, disable_paralell)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_logits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_lm_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mlog_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_lm_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(results)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-15T03:26:06.103584Z","iopub.status.idle":"2023-03-15T03:26:06.104384Z","shell.execute_reply.started":"2023-03-15T03:26:06.104103Z","shell.execute_reply":"2023-03-15T03:26:06.104131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Rating-prediction Scenario\n___","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ntargets = target_details['target prompt'] # all possible selections\ntargets_ids = target_details['movie_id']\ncontext_selection = sampled_prompt_movilens_dataset.map(map2ctx, \n                                                        fn_kwargs = {'test_scenario':test_scenario, 'targets_ids': targets_ids},\n                                                        remove_columns=sampled_prompt_movilens_dataset.column_names)\nexpected = np.array(context_selection['user_rating'])\ncontext_selection","metadata":{"execution":{"iopub.status.busy":"2023-03-15T03:26:13.240434Z","iopub.execute_input":"2023-03-15T03:26:13.241051Z","iopub.status.idle":"2023-03-15T03:26:13.598919Z","shell.execute_reply.started":"2023-03-15T03:26:13.241012Z","shell.execute_reply":"2023-03-15T03:26:13.598194Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4834017d53ce479fbd32dddd37239fd4"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['user_rating', 'biography only', 'bio + top 10 nearest ratings', 'bio + 10 nearest ratings + history overview', 'bio + 10 nearest ratings + history overview + age-group', 'bio + 10 nearest ratings + history overview + age-group + next-category', 'expected'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration, GPT2Tokenizer, GPT2LMHeadModel\nfrom metrics import *\nfrom tqdm import tqdm\nfrom BARTRec import rank_with_bart\nfrom GPTRec import rank_with_gpt\nfrom env_config import device\n\n\navailable_selections = RATINGS\nrank_method_names = ['BART', 'GPT']\nrank_methods = [rank_with_bart, rank_with_gpt]\nmodels = [BartForConditionalGeneration.from_pretrained('facebook/bart-base').to(device), GPT2LMHeadModel.from_pretrained('gpt2').to(device)]\ntokenizers = [BartTokenizer.from_pretrained('facebook/bart-base'), GPT2Tokenizer.from_pretrained('gpt2')]\nresults = []\ntotal = len(test_scenario) * len(rank_methods) * len(context_selection)\n\nwith tqdm(test_scenario, total=total, desc='Processing') as process:\n    for scenario in process:\n        if scenario in context_selection.column_names:\n            contexts = context_selection[scenario]\n            for rank_method, model, tokenizer, name in zip(rank_methods, models, tokenizers, rank_method_names):\n                predictions = rank_method(model=model, tokenizer=tokenizer,\n                                    contexts=contexts, available_selections=available_selections,\n                                    verbose=process,\n                                    batch_size=8,\n                                    term_to_estimate = None)\n                predictions = predictions[..., 0] + 1 # convert from index to rating\n                eval_rs = rmse(predictions, expected)\n                results.append({'model': name, 'scenario': scenario, **eval_rs})\nlen(results)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-15T03:26:13.600927Z","iopub.execute_input":"2023-03-15T03:26:13.601624Z","iopub.status.idle":"2023-03-15T03:44:13.780228Z","shell.execute_reply.started":"2023-03-15T03:26:13.601584Z","shell.execute_reply":"2023-03-15T03:44:13.778435Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Processing:  90%|█████████ | 9002/10000 [12:37<00:48, 20.62it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1024). Running this sequence through the model will result in indexing errors\nProcessing:   0%|          | 5/10000 [17:53<595:58:26, 214.66s/it]\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(results)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-15T03:44:13.783260Z","iopub.execute_input":"2023-03-15T03:44:13.784183Z","iopub.status.idle":"2023-03-15T03:44:13.803384Z","shell.execute_reply.started":"2023-03-15T03:44:13.784110Z","shell.execute_reply":"2023-03-15T03:44:13.802221Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"  model                                           scenario      rmse\n0  BART                                     biography only  1.222293\n1   GPT                                     biography only  2.287575\n2  BART                       bio + top 10 nearest ratings  2.493191\n3   GPT                       bio + top 10 nearest ratings  1.850405\n4  BART        bio + 10 nearest ratings + history overview  2.390397\n5   GPT        bio + 10 nearest ratings + history overview  2.252554\n6  BART  bio + 10 nearest ratings + history overview + ...  2.477701\n7   GPT  bio + 10 nearest ratings + history overview + ...  2.527251\n8  BART  bio + 10 nearest ratings + history overview + ...  2.403955\n9   GPT  bio + 10 nearest ratings + history overview + ...  2.207261","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>scenario</th>\n      <th>rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BART</td>\n      <td>biography only</td>\n      <td>1.222293</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>biography only</td>\n      <td>2.287575</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BART</td>\n      <td>bio + top 10 nearest ratings</td>\n      <td>2.493191</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GPT</td>\n      <td>bio + top 10 nearest ratings</td>\n      <td>1.850405</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BART</td>\n      <td>bio + 10 nearest ratings + history overview</td>\n      <td>2.390397</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GPT</td>\n      <td>bio + 10 nearest ratings + history overview</td>\n      <td>2.252554</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>BART</td>\n      <td>bio + 10 nearest ratings + history overview + ...</td>\n      <td>2.477701</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GPT</td>\n      <td>bio + 10 nearest ratings + history overview + ...</td>\n      <td>2.527251</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>BART</td>\n      <td>bio + 10 nearest ratings + history overview + ...</td>\n      <td>2.403955</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GPT</td>\n      <td>bio + 10 nearest ratings + history overview + ...</td>\n      <td>2.207261</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}